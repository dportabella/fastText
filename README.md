# Quick and dirty fork of fastText for debugging predict ngrams and vectors

```
$ git clone https://github.com/dportabella/fastText.git
$ cd fastText
$ git checkout debug

$ cat train.txt
__label__house bed
__label__house kitchen
__label__game football
__label__game basket

$ cat test.txt
bed
kitchen
football
basket
pine
zzzz

$ make
```


## Example1
```
$ ./fasttext supervised -minn 1 -maxn 1 -wordNgrams 0 -epoch 500 -lr 0.5 -input train.txt -output model

$ ./fasttext predict-prob model.bin test.txt 2

+++ predictLine

- token(bed, 1) {getLine->addSubwords}  {out_of_vocab: no}  { subwords: b e d  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: bed	hash: 1	vector square norm: 2.94656	vector: 0.140025 0.00791528 0.0945921 -0.0255256 -0.123936 -0.0108343 0.000203402 0.131352 -0.0804274 -0.188668 -0.0636158 -0.25928 0.127626 0.115308 0.0208016 -0.0600622 -0.136194 0.0449085 -0.389689 -0.259524 -0.220197 0.0606508 0.215346 0.092192 0.101823 0.0398224 -0.274465 -0.241057 -0.0912692 -0.0332416 0.23611 0.0324354 -0.0824725 0.01441 -0.0995954 -0.327762 -0.145957 -0.0132101 0.181059 -0.0828961 -0.172481 0.329502 0.0158459 -0.384847 0.0559681 0.290157 -0.376467 -0.23149 0.187868 0.0396027 0.147746 0.116823 0.0581595 0.0470299 -0.171821 0.0602164 0.09536 -0.185343 0.230099 -0.0632732 0.184994 -0.184245 0.171113 0.175694 0.115458 -0.174495 -0.350281 -0.109312 -0.0281675 -0.0591041 0.078666 -0.0423218 -0.0359203 -0.203819 -0.2873 -0.0900357 0.127077 -0.0775046 0.521104 -0.050889 -0.118369 -0.113206 0.00975774 -0.0883405 0.295349 -0.134529 -0.0219679 -0.253678 -0.0650749 0.259786 -0.0418825 0.137149 0.0499264 0.0216191 0.190482 -0.22464 0.148591 0.0267272 -0.140442 -0.186244 ]
ngram: b	hash: 335082	vector square norm: 0.613781	vector: -0.0607697 0.000896247 -0.0479064 0.00374918 0.0628462 0.0112739 -0.0113703 -0.0590676 0.037007 0.0899231 0.0295438 0.112377 -0.0563411 -0.0499632 -0.0106319 0.0308421 0.0560358 -0.025548 0.18281 0.122241 0.0971738 -0.0289144 -0.0989765 -0.0426783 -0.050718 -0.0203312 0.125659 0.110332 0.050125 0.0193385 -0.103184 -0.0171943 0.0344639 -0.0171323 0.0389264 0.147884 0.0593789 0.00498351 -0.0887331 0.044331 0.0670681 -0.143565 -0.00208465 0.173021 -0.0280172 -0.141549 0.178977 0.0990282 -0.0895242 -0.0192918 -0.0635075 -0.0596512 -0.03026 -0.0279327 0.0738568 -0.03243 -0.0495794 0.0895935 -0.10518 0.0238915 -0.0894459 0.0916603 -0.0784876 -0.0745993 -0.0530163 0.0784273 0.15519 0.0429365 0.00402176 0.0258614 -0.0365725 0.0189773 0.018602 0.0957972 0.133716 0.0442672 -0.0697726 0.0339207 -0.235631 0.0335401 0.064866 0.051104 -0.00119038 0.0407992 -0.129213 0.0617931 0.00527702 0.106783 0.0225565 -0.108001 0.0179882 -0.0701443 -0.0239804 -0.010647 -0.0771151 0.10696 -0.0735839 -0.0071109 0.0665023 0.0778363 ]
ngram: e	hash: 891749	vector square norm: 1.67341	vector: 0.0962932 0.0043802 0.0736295 -0.0150743 -0.0951058 -0.014539 0.0146226 0.095285 -0.0662469 -0.143954 -0.0497558 -0.18637 0.0927283 0.0816166 0.0288216 -0.0416271 -0.100454 0.0252866 -0.292181 -0.203267 -0.166384 0.0362609 0.152516 0.0647406 0.0799986 0.0407803 -0.18992 -0.188094 -0.0750006 -0.0188861 0.185696 0.0162367 -0.0727935 0.0158307 -0.0669281 -0.237906 -0.112159 -0.0221323 0.138339 -0.0673749 -0.129141 0.24325 0.0142656 -0.298933 0.0293424 0.223482 -0.295933 -0.178104 0.13813 0.0209466 0.114013 0.0925986 0.0447805 0.0344636 -0.127711 0.0333485 0.0759575 -0.14244 0.174997 -0.0518142 0.133897 -0.143693 0.117515 0.136849 0.0784507 -0.136018 -0.265934 -0.0863082 -0.0203393 -0.0378061 0.0631712 -0.0328202 -0.0356413 -0.150238 -0.21181 -0.0678325 0.104958 -0.0630769 0.396576 -0.048814 -0.097015 -0.0817625 0.00613349 -0.0678604 0.217583 -0.107802 -0.0173628 -0.185115 -0.0451742 0.187445 -0.0416783 0.109618 0.0276893 0.0114261 0.140214 -0.171124 0.113816 0.0269887 -0.104443 -0.131381 ]
ngram: d	hash: 1669368	vector square norm: 2.91215	vector: 0.140898 -0.00618392 0.0929265 -0.0135712 -0.137087 -0.0172198 -0.00193382 0.124348 -0.0722593 -0.180481 -0.0545874 -0.253775 0.133211 0.120225 0.0332556 -0.0460404 -0.13929 0.0448833 -0.387346 -0.264781 -0.224594 0.0530011 0.213595 0.0808083 0.117678 0.051971 -0.258345 -0.247955 -0.0849766 -0.0327293 0.240655 0.0323587 -0.0946118 0.0205624 -0.0852278 -0.327551 -0.13344 -0.0267375 0.183943 -0.0849765 -0.157213 0.315514 0.0288183 -0.386686 0.0554635 0.294387 -0.385899 -0.236035 0.199157 0.0278412 0.157711 0.126578 0.0592413 0.0355292 -0.167834 0.0530075 0.109563 -0.188063 0.226481 -0.0542209 0.189278 -0.193776 0.160659 0.157885 0.105622 -0.174577 -0.337878 -0.111529 -0.0243478 -0.0467069 0.0740374 -0.0529596 -0.0413813 -0.18911 -0.285382 -0.0874791 0.12831 -0.0806045 0.522116 -0.0635454 -0.115688 -0.0989168 0.00825774 -0.100713 0.29897 -0.138988 -0.0299366 -0.244175 -0.0516736 0.256788 -0.0382955 0.130828 0.0508358 0.0291527 0.183961 -0.231231 0.14747 0.0233844 -0.126143 -0.172571 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__house 0.999455 __label__game 0.000564705

+++ predictLine

- token(kitchen, 2) {getLine->addSubwords}  {out_of_vocab: no}  { subwords: k i t c h e n  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: kitchen	hash: 2	vector square norm: 2.03882	vector: 0.121137 -0.0051069 0.0715844 -0.0186723 -0.106932 -0.0219452 -0.00359575 0.0984186 -0.0536666 -0.144747 -0.0620938 -0.213051 0.10849 0.0895371 0.0202119 -0.0390712 -0.113369 0.0300568 -0.327041 -0.214228 -0.173666 0.0366361 0.169855 0.0734854 0.0916781 0.0368936 -0.21401 -0.210667 -0.0826906 -0.0188416 0.203591 0.0334567 -0.0788201 0.0182184 -0.0874333 -0.278217 -0.123912 -0.00798214 0.159508 -0.0597407 -0.144057 0.273164 0.0232286 -0.323764 0.0430994 0.24534 -0.312517 -0.18475 0.164752 0.0366622 0.122938 0.0996215 0.0512417 0.0293367 -0.147354 0.053341 0.0882329 -0.163575 0.17962 -0.055091 0.162629 -0.147464 0.144568 0.147449 0.084529 -0.140262 -0.293785 -0.0910091 -0.0153618 -0.0527483 0.0708732 -0.0351771 -0.0315069 -0.17314 -0.246315 -0.0703681 0.118709 -0.0703359 0.439718 -0.0554302 -0.0910718 -0.0911961 0.000745754 -0.0824829 0.240626 -0.123474 -0.0208997 -0.19499 -0.0438099 0.207598 -0.0429527 0.112082 0.0469198 0.0129327 0.156454 -0.191239 0.121886 0.0279524 -0.106053 -0.140772 ]
ngram: k	hash: 1778415	vector square norm: 0.183862	vector: -0.0433265 -0.0100854 -0.0276568 -0.00570725 0.0367605 0.0135529 -0.00592355 -0.0306194 0.0178139 0.0476302 0.0200236 0.0720419 -0.0388765 -0.0326549 0.00117178 0.00945708 0.0434601 -0.00507435 0.0877331 0.0696919 0.0609396 -0.0191874 -0.0465324 -0.0143573 -0.032173 -0.0113946 0.0560095 0.068048 0.018575 0.011904 -0.063378 -0.0099087 0.0129999 -0.00771192 0.0138285 0.0758474 0.0422022 -0.00531843 -0.0472566 0.015509 0.0376773 -0.0798744 -0.00295847 0.0970538 -0.00560828 -0.0719057 0.0986558 0.04806 -0.0530401 -0.00807169 -0.0297743 -0.0238001 -0.0110083 -0.00442458 0.0323307 -0.00918709 -0.031566 0.0429103 -0.055695 0.0207565 -0.0490866 0.0444367 -0.0441096 -0.0496264 -0.0356276 0.0461858 0.0894117 0.0249376 0.00173948 0.00519467 -0.0301603 0.019685 0.0188619 0.0410586 0.0745784 0.0179067 -0.0280533 0.0178986 -0.135912 0.014303 0.027483 0.0239444 0.00681526 0.0217747 -0.0688312 0.0363726 0.00216414 0.0618336 0.0241783 -0.055338 0.00804517 -0.0419139 -0.0152246 -0.00750291 -0.0378194 0.0579112 -0.0345394 -0.00180907 0.0416771 0.0363098 ]
ngram: i	hash: 223177	vector square norm: 2.04729	vector: 0.118312 0.00378389 0.0665222 -0.00767168 -0.106089 -0.0158022 -0.000138476 0.114947 -0.0547025 -0.151036 -0.0426991 -0.214421 0.117983 0.095029 0.0306573 -0.0343082 -0.107125 0.0274153 -0.318735 -0.213259 -0.184339 0.0505264 0.178555 0.0718639 0.101339 0.0304034 -0.226832 -0.204502 -0.0651228 -0.0271916 0.206873 0.0192429 -0.0722212 0.0135979 -0.0715009 -0.27734 -0.115183 -0.0159794 0.15762 -0.0583104 -0.135436 0.27545 0.0199833 -0.328253 0.042952 0.235125 -0.330156 -0.188621 0.167645 0.0328069 0.129442 0.0928168 0.049408 0.0385102 -0.136689 0.0450162 0.0785833 -0.16214 0.186877 -0.0441101 0.153017 -0.161105 0.144472 0.135398 0.092699 -0.148047 -0.296136 -0.0868214 -0.0236096 -0.0491835 0.0646626 -0.0321521 -0.0405789 -0.158477 -0.243502 -0.0718295 0.116179 -0.0667358 0.427495 -0.0422973 -0.106333 -0.0954821 0.0139826 -0.0740106 0.251573 -0.119972 -0.0253623 -0.211729 -0.0512943 0.205013 -0.0274654 0.102909 0.0337411 0.020866 0.160867 -0.194464 0.114668 0.0133141 -0.121244 -0.152814 ]
ngram: t	hash: 111272	vector square norm: 1.15589	vector: -0.087417 -0.00916161 -0.0496416 0.00214341 0.0793724 0.0126973 -0.00236421 -0.0891492 0.0558969 0.111419 0.0365188 0.163104 -0.0816646 -0.0635446 -0.0258204 0.0307842 0.0826965 -0.017513 0.241828 0.161227 0.126955 -0.0328882 -0.128203 -0.0474052 -0.0749266 -0.0239091 0.17357 0.158426 0.0512094 0.012906 -0.160467 -0.0186447 0.0634632 -0.00857917 0.0576733 0.211892 0.0939939 0.0128043 -0.118022 0.0484567 0.102622 -0.202618 -0.0207809 0.251368 -0.0306068 -0.183734 0.238492 0.13605 -0.122889 -0.0194502 -0.0905502 -0.0732709 -0.0353091 -0.0232906 0.103621 -0.0350835 -0.0712131 0.109374 -0.144369 0.0323849 -0.121751 0.115531 -0.0975247 -0.112304 -0.0767274 0.0970142 0.209366 0.0585102 0.0225534 0.0322295 -0.0470049 0.0349526 0.0212938 0.122796 0.17846 0.0478357 -0.08227 0.0565457 -0.329352 0.0336223 0.07502 0.0654688 -0.00305579 0.0599796 -0.190701 0.0854309 0.00471014 0.160877 0.0451569 -0.158434 0.0285531 -0.087762 -0.0248901 -0.00603825 -0.113946 0.14997 -0.101084 -0.026804 0.0793734 0.117839 ]
ngram: c	hash: 1557463	vector square norm: 2.03039	vector: 0.11745 0.0112764 0.0709516 -0.0250281 -0.0979267 -0.0118429 0.0127998 0.110856 -0.0722492 -0.15386 -0.0528392 -0.220576 0.108744 0.0996501 0.0173432 -0.0470099 -0.121876 0.0252888 -0.317665 -0.22423 -0.173779 0.0434861 0.183324 0.0752126 0.091951 0.0401712 -0.214586 -0.194539 -0.0751739 -0.0202899 0.195665 0.032766 -0.0711159 0.0221934 -0.0811714 -0.264647 -0.116532 -0.0175721 0.151607 -0.0710619 -0.133067 0.263802 0.0187176 -0.326911 0.0436298 0.245003 -0.327506 -0.189679 0.160274 0.0363407 0.127196 0.0979645 0.0411046 0.0275205 -0.142993 0.0518065 0.0916574 -0.164158 0.183856 -0.0450518 0.143862 -0.161959 0.130388 0.137865 0.0873894 -0.137915 -0.297818 -0.0915998 -0.021169 -0.0402597 0.0646803 -0.0311015 -0.0325548 -0.157236 -0.229168 -0.0805142 0.110396 -0.0678158 0.437243 -0.0470195 -0.101553 -0.09495 0.0160656 -0.0837764 0.246992 -0.121659 -0.0163419 -0.213682 -0.0508554 0.215909 -0.0416571 0.119305 0.0479199 0.006131 0.149328 -0.1866 0.118652 0.0304426 -0.116959 -0.150874 ]
ngram: h	hash: 1000796	vector square norm: 2.02398	vector: 0.12219 0.0041294 0.0723635 -0.0123151 -0.0994318 -0.0142813 -0.00157823 0.117054 -0.0590534 -0.156681 -0.0574252 -0.215828 0.113215 0.0984456 0.0233002 -0.0426391 -0.106074 0.0413681 -0.312105 -0.223044 -0.173697 0.0502803 0.176121 0.0703092 0.100186 0.0290182 -0.219863 -0.196342 -0.07756 -0.022455 0.205553 0.0347923 -0.0805409 0.0144052 -0.0837826 -0.277203 -0.109628 -0.00704898 0.146946 -0.0759873 -0.139811 0.274736 0.00992024 -0.329195 0.0438464 0.236856 -0.313612 -0.186352 0.161245 0.0216598 0.116384 0.0941679 0.0389258 0.0369744 -0.139636 0.0505962 0.0862604 -0.148762 0.196114 -0.0581157 0.151257 -0.163888 0.13332 0.135675 0.0878303 -0.134305 -0.296664 -0.0923285 -0.0264538 -0.0431465 0.0683528 -0.0337058 -0.0287681 -0.16306 -0.244678 -0.0813922 0.116026 -0.0756259 0.431508 -0.0546689 -0.106783 -0.0796805 0.0136036 -0.0720009 0.235928 -0.125434 -0.0230003 -0.204614 -0.0515584 0.212061 -0.0329186 0.120922 0.0306765 0.0231319 0.147588 -0.189526 0.115636 0.0290423 -0.114915 -0.158167 ]
ngram: e	hash: 891749	vector square norm: 1.67341	vector: 0.0962932 0.0043802 0.0736295 -0.0150743 -0.0951058 -0.014539 0.0146226 0.095285 -0.0662469 -0.143954 -0.0497558 -0.18637 0.0927283 0.0816166 0.0288216 -0.0416271 -0.100454 0.0252866 -0.292181 -0.203267 -0.166384 0.0362609 0.152516 0.0647406 0.0799986 0.0407803 -0.18992 -0.188094 -0.0750006 -0.0188861 0.185696 0.0162367 -0.0727935 0.0158307 -0.0669281 -0.237906 -0.112159 -0.0221323 0.138339 -0.0673749 -0.129141 0.24325 0.0142656 -0.298933 0.0293424 0.223482 -0.295933 -0.178104 0.13813 0.0209466 0.114013 0.0925986 0.0447805 0.0344636 -0.127711 0.0333485 0.0759575 -0.14244 0.174997 -0.0518142 0.133897 -0.143693 0.117515 0.136849 0.0784507 -0.136018 -0.265934 -0.0863082 -0.0203393 -0.0378061 0.0631712 -0.0328202 -0.0356413 -0.150238 -0.21181 -0.0678325 0.104958 -0.0630769 0.396576 -0.048814 -0.097015 -0.0817625 0.00613349 -0.0678604 0.217583 -0.107802 -0.0173628 -0.185115 -0.0451742 0.187445 -0.0416783 0.109618 0.0276893 0.0114261 0.140214 -0.171124 0.113816 0.0269887 -0.104443 -0.131381 ]
ngram: n	hash: 1445558	vector square norm: 2.03273	vector: 0.115328 -0.00159127 0.0781617 -0.00968126 -0.109293 -0.0230997 0.0111041 0.112796 -0.0642889 -0.161002 -0.0505491 -0.202014 0.101984 0.0960146 0.028831 -0.0451768 -0.125733 0.0306294 -0.322037 -0.212361 -0.17347 0.0332011 0.167755 0.0793216 0.0888288 0.0476345 -0.224375 -0.197683 -0.0695378 -0.0143966 0.202109 0.0266177 -0.0646274 0.0100877 -0.0849254 -0.266805 -0.126351 -0.0138007 0.143885 -0.0665855 -0.145234 0.275908 0.0209324 -0.326265 0.0368046 0.243728 -0.327991 -0.188467 0.16454 0.0302342 0.122538 0.100268 0.0393594 0.0349506 -0.128555 0.0381871 0.0804341 -0.160807 0.179728 -0.0507491 0.162375 -0.149587 0.146387 0.13092 0.0966589 -0.136515 -0.288352 -0.0970653 -0.0135784 -0.0498898 0.0659459 -0.0328435 -0.0337791 -0.162037 -0.243714 -0.0697479 0.115322 -0.0676911 0.43678 -0.0377829 -0.106804 -0.0880988 -0.000832913 -0.0746874 0.241568 -0.119418 -0.0230651 -0.209441 -0.0493823 0.204185 -0.0427221 0.118767 0.048137 0.0140719 0.155568 -0.198925 0.132848 0.0256785 -0.115287 -0.155781 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__house 0.998991 __label__game 0.00102913

+++ predictLine

- token(football, 4) {getLine->addSubwords}  {out_of_vocab: no}  { subwords: f o o t b a l l  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: football	hash: 4	vector square norm: 0.425729	vector: -0.0554728 0.00367623 -0.0324633 0.00908252 0.0569767 0.0157402 0.0021998 -0.0582294 0.0334222 0.0733621 0.0233092 0.0900968 -0.0544478 -0.04075 -0.00197053 0.0204979 0.0547369 -0.0172264 0.138596 0.107673 0.0896004 -0.0225097 -0.0841594 -0.0417178 -0.0386795 -0.0134031 0.107731 0.0945337 0.0413348 0.0159939 -0.0849855 -0.0206821 0.0261554 -0.00837249 0.0373317 0.129023 0.0434167 0.0174436 -0.061917 0.0318836 0.0581212 -0.124269 -0.000767138 0.155513 -0.0131243 -0.118615 0.143971 0.0958808 -0.0732084 -0.0164452 -0.0612926 -0.0400252 -0.0159091 -0.0156132 0.0721682 -0.0128512 -0.0433765 0.0757892 -0.0937581 0.0321591 -0.0653485 0.0720504 -0.0667166 -0.0643308 -0.032696 0.0699118 0.126655 0.0320936 0.00308699 0.0241547 -0.0328424 0.0153202 0.00827397 0.0806282 0.102123 0.0296105 -0.0575779 0.0201873 -0.189875 0.0192893 0.0379322 0.0418682 -0.00366082 0.0385481 -0.109891 0.0506031 0.0117972 0.097881 0.0177752 -0.0875202 0.0221272 -0.0503214 -0.00930399 -0.00744813 -0.0673424 0.0936188 -0.0558862 -0.0199461 0.0446173 0.0616267 ]
ngram: f	hash: 1224606	vector square norm: 0.432405	vector: -0.0487721 -0.00231035 -0.0328836 0.00749058 0.0507826 0.00459617 -0.00350203 -0.0582695 0.0369151 0.0713663 0.0189962 0.092631 -0.0541889 -0.0375667 -0.0123233 0.0291572 0.0618832 -0.0111119 0.136642 0.106031 0.0877248 -0.0168457 -0.0872518 -0.0318315 -0.0368551 -0.025366 0.0915877 0.0893617 0.0419885 0.00733196 -0.0955722 -0.00692285 0.0290014 -0.00506774 0.0342492 0.126996 0.0549992 0.0126608 -0.0729158 0.0332294 0.0589953 -0.11622 -0.0108331 0.13813 -0.0231746 -0.103213 0.15115 0.0919329 -0.0803327 -0.0106402 -0.0496107 -0.0465678 -0.0278826 -0.012514 0.0713228 -0.0209118 -0.0311506 0.0748071 -0.093331 0.0294903 -0.0763006 0.0657038 -0.0713694 -0.0591537 -0.0459281 0.0744495 0.139457 0.0332468 0.0149244 0.0119733 -0.0333871 0.00770671 0.0127221 0.0816902 0.11692 0.0398508 -0.0585979 0.0292423 -0.198716 0.0220165 0.0513893 0.0414304 -0.0102923 0.0437066 -0.111788 0.049248 0.00930187 0.0835373 0.0223932 -0.103354 0.0154954 -0.0604256 -0.01554 -0.00267292 -0.0637009 0.0902891 -0.0515882 -0.0163385 0.0543867 0.0736367 ]
ngram: o	hash: 667939	vector square norm: 1.70613	vector: -0.108098 -0.00984586 -0.0647253 0.0125573 0.10278 0.0111432 -0.0141054 -0.101853 0.055803 0.145315 0.0554454 0.196666 -0.100561 -0.0840004 -0.014955 0.0381991 0.108933 -0.0217123 0.298407 0.204507 0.165664 -0.0351158 -0.166644 -0.0742528 -0.0784522 -0.0337184 0.188614 0.185295 0.0751901 0.0137987 -0.190337 -0.02696 0.0583974 -0.0218912 0.0723971 0.249889 0.106333 0.0160411 -0.132739 0.0549725 0.118152 -0.251199 -0.0172305 0.300276 -0.0260706 -0.21582 0.295512 0.174097 -0.144568 -0.0362004 -0.109223 -0.0968988 -0.0389467 -0.0398599 0.116485 -0.0483034 -0.0852051 0.148812 -0.170393 0.0451185 -0.134125 0.150136 -0.123582 -0.134662 -0.0885018 0.122492 0.270252 0.0838421 0.0132424 0.0342246 -0.0655267 0.0422992 0.033921 0.152754 0.211256 0.0683536 -0.0971758 0.0598179 -0.396404 0.0397258 0.0881232 0.073235 -0.00967391 0.062883 -0.217421 0.107166 0.0141311 0.192481 0.0529316 -0.199197 0.036091 -0.11084 -0.0459279 -0.0191393 -0.149202 0.180067 -0.111867 -0.0126491 0.107737 0.137985 ]
ngram: o	hash: 667939	vector square norm: 1.70613	vector: -0.108098 -0.00984586 -0.0647253 0.0125573 0.10278 0.0111432 -0.0141054 -0.101853 0.055803 0.145315 0.0554454 0.196666 -0.100561 -0.0840004 -0.014955 0.0381991 0.108933 -0.0217123 0.298407 0.204507 0.165664 -0.0351158 -0.166644 -0.0742528 -0.0784522 -0.0337184 0.188614 0.185295 0.0751901 0.0137987 -0.190337 -0.02696 0.0583974 -0.0218912 0.0723971 0.249889 0.106333 0.0160411 -0.132739 0.0549725 0.118152 -0.251199 -0.0172305 0.300276 -0.0260706 -0.21582 0.295512 0.174097 -0.144568 -0.0362004 -0.109223 -0.0968988 -0.0389467 -0.0398599 0.116485 -0.0483034 -0.0852051 0.148812 -0.170393 0.0451185 -0.134125 0.150136 -0.123582 -0.134662 -0.0885018 0.122492 0.270252 0.0838421 0.0132424 0.0342246 -0.0655267 0.0422992 0.033921 0.152754 0.211256 0.0683536 -0.0971758 0.0598179 -0.396404 0.0397258 0.0881232 0.073235 -0.00967391 0.062883 -0.217421 0.107166 0.0141311 0.192481 0.0529316 -0.199197 0.036091 -0.11084 -0.0459279 -0.0191393 -0.149202 0.180067 -0.111867 -0.0126491 0.107737 0.137985 ]
ngram: t	hash: 111272	vector square norm: 1.15589	vector: -0.087417 -0.00916161 -0.0496416 0.00214341 0.0793724 0.0126973 -0.00236421 -0.0891492 0.0558969 0.111419 0.0365188 0.163104 -0.0816646 -0.0635446 -0.0258204 0.0307842 0.0826965 -0.017513 0.241828 0.161227 0.126955 -0.0328882 -0.128203 -0.0474052 -0.0749266 -0.0239091 0.17357 0.158426 0.0512094 0.012906 -0.160467 -0.0186447 0.0634632 -0.00857917 0.0576733 0.211892 0.0939939 0.0128043 -0.118022 0.0484567 0.102622 -0.202618 -0.0207809 0.251368 -0.0306068 -0.183734 0.238492 0.13605 -0.122889 -0.0194502 -0.0905502 -0.0732709 -0.0353091 -0.0232906 0.103621 -0.0350835 -0.0712131 0.109374 -0.144369 0.0323849 -0.121751 0.115531 -0.0975247 -0.112304 -0.0767274 0.0970142 0.209366 0.0585102 0.0225534 0.0322295 -0.0470049 0.0349526 0.0212938 0.122796 0.17846 0.0478357 -0.08227 0.0565457 -0.329352 0.0336223 0.07502 0.0654688 -0.00305579 0.0599796 -0.190701 0.0854309 0.00471014 0.160877 0.0451569 -0.158434 0.0285531 -0.087762 -0.0248901 -0.00603825 -0.113946 0.14997 -0.101084 -0.026804 0.0793734 0.117839 ]
ngram: b	hash: 335082	vector square norm: 0.613781	vector: -0.0607697 0.000896247 -0.0479064 0.00374918 0.0628462 0.0112739 -0.0113703 -0.0590676 0.037007 0.0899231 0.0295438 0.112377 -0.0563411 -0.0499632 -0.0106319 0.0308421 0.0560358 -0.025548 0.18281 0.122241 0.0971738 -0.0289144 -0.0989765 -0.0426783 -0.050718 -0.0203312 0.125659 0.110332 0.050125 0.0193385 -0.103184 -0.0171943 0.0344639 -0.0171323 0.0389264 0.147884 0.0593789 0.00498351 -0.0887331 0.044331 0.0670681 -0.143565 -0.00208465 0.173021 -0.0280172 -0.141549 0.178977 0.0990282 -0.0895242 -0.0192918 -0.0635075 -0.0596512 -0.03026 -0.0279327 0.0738568 -0.03243 -0.0495794 0.0895935 -0.10518 0.0238915 -0.0894459 0.0916603 -0.0784876 -0.0745993 -0.0530163 0.0784273 0.15519 0.0429365 0.00402176 0.0258614 -0.0365725 0.0189773 0.018602 0.0957972 0.133716 0.0442672 -0.0697726 0.0339207 -0.235631 0.0335401 0.064866 0.051104 -0.00119038 0.0407992 -0.129213 0.0617931 0.00527702 0.106783 0.0225565 -0.108001 0.0179882 -0.0701443 -0.0239804 -0.010647 -0.0771151 0.10696 -0.0735839 -0.0071109 0.0665023 0.0778363 ]
ngram: a	hash: 2225	vector square norm: 6.25256	vector: -0.201802 -0.00408085 -0.115747 0.0294652 0.189497 0.0166203 -0.0104516 -0.184725 0.107079 0.278376 0.0945558 0.366294 -0.195155 -0.168061 -0.0477137 0.0865234 0.21219 -0.0599867 0.568488 0.373866 0.314468 -0.0670822 -0.316796 -0.122043 -0.159956 -0.0740003 0.390977 0.350654 0.140871 0.0377933 -0.360528 -0.0499929 0.13164 -0.025747 0.140849 0.474817 0.208691 0.0239393 -0.269371 0.118332 0.248421 -0.471024 -0.0295031 0.575881 -0.0611769 -0.414072 0.566031 0.342307 -0.284105 -0.0614583 -0.226923 -0.171347 -0.0832122 -0.0605452 0.238576 -0.0772612 -0.155736 0.2718 -0.319039 0.0936117 -0.269689 0.28147 -0.246874 -0.250506 -0.15656 0.257736 0.509715 0.15822 0.0285071 0.0861013 -0.103793 0.07082 0.0593413 0.294374 0.416398 0.128398 -0.196699 0.113902 -0.760305 0.0847362 0.179713 0.164684 -0.0138663 0.127425 -0.433757 0.198694 0.0285449 0.35633 0.0964198 -0.377505 0.0645183 -0.18831 -0.0621845 -0.0365134 -0.262375 0.330871 -0.225837 -0.0308584 0.191582 0.265542 ]
ngram: l	hash: 1890320	vector square norm: 1.68457	vector: -0.0954978 -0.00732521 -0.0580819 0.0101227 0.0982404 0.0139819 0.00500805 -0.103827 0.0608272 0.131287 0.0564629 0.195735 -0.104187 -0.0963513 -0.0242388 0.0363344 0.108948 -0.0339268 0.296258 0.202293 0.166728 -0.0480689 -0.163712 -0.0625355 -0.092555 -0.0344213 0.204257 0.188286 0.061199 0.0167389 -0.18549 -0.0317623 0.0721428 -0.0160914 0.0704355 0.252725 0.101528 0.017721 -0.13041 0.0675609 0.126965 -0.245308 -0.0120529 0.298023 -0.0322387 -0.226632 0.298467 0.183083 -0.13708 -0.022591 -0.111177 -0.0933445 -0.0474762 -0.0277479 0.119071 -0.0364457 -0.0695577 0.134687 -0.168418 0.0362668 -0.144082 0.136436 -0.126972 -0.117453 -0.0818131 0.131329 0.258782 0.0869029 0.0271531 0.0396031 -0.0672036 0.0357379 0.0397312 0.144664 0.223683 0.0746876 -0.0981435 0.0638374 -0.38489 0.0433593 0.0931038 0.0857539 -0.00591107 0.0678731 -0.225973 0.117166 0.00862752 0.180077 0.048977 -0.190879 0.0291512 -0.0999202 -0.040332 -0.00757616 -0.137824 0.178908 -0.111097 -0.0134107 0.109263 0.130764 ]
ngram: l	hash: 1890320	vector square norm: 1.68457	vector: -0.0954978 -0.00732521 -0.0580819 0.0101227 0.0982404 0.0139819 0.00500805 -0.103827 0.0608272 0.131287 0.0564629 0.195735 -0.104187 -0.0963513 -0.0242388 0.0363344 0.108948 -0.0339268 0.296258 0.202293 0.166728 -0.0480689 -0.163712 -0.0625355 -0.092555 -0.0344213 0.204257 0.188286 0.061199 0.0167389 -0.18549 -0.0317623 0.0721428 -0.0160914 0.0704355 0.252725 0.101528 0.017721 -0.13041 0.0675609 0.126965 -0.245308 -0.0120529 0.298023 -0.0322387 -0.226632 0.298467 0.183083 -0.13708 -0.022591 -0.111177 -0.0933445 -0.0474762 -0.0277479 0.119071 -0.0364457 -0.0695577 0.134687 -0.168418 0.0362668 -0.144082 0.136436 -0.126972 -0.117453 -0.0818131 0.131329 0.258782 0.0869029 0.0271531 0.0396031 -0.0672036 0.0357379 0.0397312 0.144664 0.223683 0.0746876 -0.0981435 0.0638374 -0.38489 0.0433593 0.0931038 0.0857539 -0.00591107 0.0678731 -0.225973 0.117166 0.00862752 0.180077 0.048977 -0.190879 0.0291512 -0.0999202 -0.040332 -0.00757616 -0.137824 0.178908 -0.111097 -0.0134107 0.109263 0.130764 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__game 0.999782 __label__house 0.000237978

+++ predictLine

- token(basket, 3) {getLine->addSubwords}  {out_of_vocab: no}  { subwords: b a s k e t  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: basket	hash: 3	vector square norm: 3.41869	vector: -0.147482 0.00542908 -0.0989988 0.0157437 0.140304 0.0274565 -0.0128659 -0.141888 0.0887188 0.206731 0.0646674 0.281826 -0.150132 -0.118753 -0.0248223 0.0515654 0.143692 -0.0498744 0.422611 0.285311 0.241514 -0.0468582 -0.229681 -0.0921842 -0.126398 -0.0450067 0.292213 0.258119 0.0886763 0.0220832 -0.254391 -0.0305717 0.0943385 -0.012803 0.100429 0.360077 0.152425 0.0173273 -0.196855 0.0922938 0.178864 -0.348545 -0.026664 0.41373 -0.0512761 -0.316677 0.423915 0.247669 -0.20186 -0.0327702 -0.170364 -0.135272 -0.058547 -0.0525803 0.17587 -0.0650609 -0.111919 0.205375 -0.236796 0.0662555 -0.207028 0.205708 -0.176134 -0.18949 -0.124706 0.187371 0.375049 0.109807 0.0237841 0.0535126 -0.0756543 0.0516925 0.0455645 0.217019 0.308248 0.10479 -0.139429 0.0958894 -0.551182 0.0540243 0.131309 0.118988 -0.00210726 0.104396 -0.305088 0.151932 0.0219964 0.271995 0.0560419 -0.282228 0.0494018 -0.152808 -0.0509341 -0.0175053 -0.20407 0.243459 -0.160041 -0.0241005 0.143596 0.196187 ]
ngram: b	hash: 335082	vector square norm: 0.613781	vector: -0.0607697 0.000896247 -0.0479064 0.00374918 0.0628462 0.0112739 -0.0113703 -0.0590676 0.037007 0.0899231 0.0295438 0.112377 -0.0563411 -0.0499632 -0.0106319 0.0308421 0.0560358 -0.025548 0.18281 0.122241 0.0971738 -0.0289144 -0.0989765 -0.0426783 -0.050718 -0.0203312 0.125659 0.110332 0.050125 0.0193385 -0.103184 -0.0171943 0.0344639 -0.0171323 0.0389264 0.147884 0.0593789 0.00498351 -0.0887331 0.044331 0.0670681 -0.143565 -0.00208465 0.173021 -0.0280172 -0.141549 0.178977 0.0990282 -0.0895242 -0.0192918 -0.0635075 -0.0596512 -0.03026 -0.0279327 0.0738568 -0.03243 -0.0495794 0.0895935 -0.10518 0.0238915 -0.0894459 0.0916603 -0.0784876 -0.0745993 -0.0530163 0.0784273 0.15519 0.0429365 0.00402176 0.0258614 -0.0365725 0.0189773 0.018602 0.0957972 0.133716 0.0442672 -0.0697726 0.0339207 -0.235631 0.0335401 0.064866 0.051104 -0.00119038 0.0407992 -0.129213 0.0617931 0.00527702 0.106783 0.0225565 -0.108001 0.0179882 -0.0701443 -0.0239804 -0.010647 -0.0771151 0.10696 -0.0735839 -0.0071109 0.0665023 0.0778363 ]
ngram: a	hash: 2225	vector square norm: 6.25256	vector: -0.201802 -0.00408085 -0.115747 0.0294652 0.189497 0.0166203 -0.0104516 -0.184725 0.107079 0.278376 0.0945558 0.366294 -0.195155 -0.168061 -0.0477137 0.0865234 0.21219 -0.0599867 0.568488 0.373866 0.314468 -0.0670822 -0.316796 -0.122043 -0.159956 -0.0740003 0.390977 0.350654 0.140871 0.0377933 -0.360528 -0.0499929 0.13164 -0.025747 0.140849 0.474817 0.208691 0.0239393 -0.269371 0.118332 0.248421 -0.471024 -0.0295031 0.575881 -0.0611769 -0.414072 0.566031 0.342307 -0.284105 -0.0614583 -0.226923 -0.171347 -0.0832122 -0.0605452 0.238576 -0.0772612 -0.155736 0.2718 -0.319039 0.0936117 -0.269689 0.28147 -0.246874 -0.250506 -0.15656 0.257736 0.509715 0.15822 0.0285071 0.0861013 -0.103793 0.07082 0.0593413 0.294374 0.416398 0.128398 -0.196699 0.113902 -0.760305 0.0847362 0.179713 0.164684 -0.0138663 0.127425 -0.433757 0.198694 0.0285449 0.35633 0.0964198 -0.377505 0.0645183 -0.18831 -0.0621845 -0.0365134 -0.262375 0.330871 -0.225837 -0.0308584 0.191582 0.265542 ]
ngram: s	hash: 1999367	vector square norm: 3.41552	vector: -0.143634 0.00340641 -0.0984032 0.0288947 0.148365 0.0235701 -0.0115277 -0.135407 0.0786563 0.208995 0.0660808 0.268344 -0.146191 -0.130106 -0.0213455 0.0590945 0.157491 -0.0385052 0.409244 0.279477 0.235195 -0.0572997 -0.229087 -0.0935074 -0.113717 -0.054196 0.293847 0.253124 0.0930477 0.0229719 -0.258993 -0.0291014 0.0989723 -0.0300956 0.11229 0.351883 0.14027 0.0266944 -0.201551 0.0836975 0.168571 -0.351106 -0.0239712 0.427989 -0.0562839 -0.318796 0.425958 0.256574 -0.205443 -0.0361257 -0.159557 -0.133097 -0.0553072 -0.0491913 0.183592 -0.067536 -0.120651 0.206814 -0.241374 0.0734358 -0.191043 0.201284 -0.18526 -0.180291 -0.116943 0.193201 0.373702 0.107407 0.0153397 0.0621632 -0.079389 0.0503385 0.0543203 0.211485 0.315023 0.0878348 -0.140874 0.0828778 -0.565208 0.0704904 0.123111 0.113672 -0.0115983 0.10654 -0.31572 0.152038 0.028334 0.258136 0.0562905 -0.271983 0.0576011 -0.137361 -0.0574185 -0.0143783 -0.199618 0.238718 -0.16766 -0.0319078 0.145151 0.20316 ]
ngram: k	hash: 1778415	vector square norm: 0.183862	vector: -0.0433265 -0.0100854 -0.0276568 -0.00570725 0.0367605 0.0135529 -0.00592355 -0.0306194 0.0178139 0.0476302 0.0200236 0.0720419 -0.0388765 -0.0326549 0.00117178 0.00945708 0.0434601 -0.00507435 0.0877331 0.0696919 0.0609396 -0.0191874 -0.0465324 -0.0143573 -0.032173 -0.0113946 0.0560095 0.068048 0.018575 0.011904 -0.063378 -0.0099087 0.0129999 -0.00771192 0.0138285 0.0758474 0.0422022 -0.00531843 -0.0472566 0.015509 0.0376773 -0.0798744 -0.00295847 0.0970538 -0.00560828 -0.0719057 0.0986558 0.04806 -0.0530401 -0.00807169 -0.0297743 -0.0238001 -0.0110083 -0.00442458 0.0323307 -0.00918709 -0.031566 0.0429103 -0.055695 0.0207565 -0.0490866 0.0444367 -0.0441096 -0.0496264 -0.0356276 0.0461858 0.0894117 0.0249376 0.00173948 0.00519467 -0.0301603 0.019685 0.0188619 0.0410586 0.0745784 0.0179067 -0.0280533 0.0178986 -0.135912 0.014303 0.027483 0.0239444 0.00681526 0.0217747 -0.0688312 0.0363726 0.00216414 0.0618336 0.0241783 -0.055338 0.00804517 -0.0419139 -0.0152246 -0.00750291 -0.0378194 0.0579112 -0.0345394 -0.00180907 0.0416771 0.0363098 ]
ngram: e	hash: 891749	vector square norm: 1.67341	vector: 0.0962932 0.0043802 0.0736295 -0.0150743 -0.0951058 -0.014539 0.0146226 0.095285 -0.0662469 -0.143954 -0.0497558 -0.18637 0.0927283 0.0816166 0.0288216 -0.0416271 -0.100454 0.0252866 -0.292181 -0.203267 -0.166384 0.0362609 0.152516 0.0647406 0.0799986 0.0407803 -0.18992 -0.188094 -0.0750006 -0.0188861 0.185696 0.0162367 -0.0727935 0.0158307 -0.0669281 -0.237906 -0.112159 -0.0221323 0.138339 -0.0673749 -0.129141 0.24325 0.0142656 -0.298933 0.0293424 0.223482 -0.295933 -0.178104 0.13813 0.0209466 0.114013 0.0925986 0.0447805 0.0344636 -0.127711 0.0333485 0.0759575 -0.14244 0.174997 -0.0518142 0.133897 -0.143693 0.117515 0.136849 0.0784507 -0.136018 -0.265934 -0.0863082 -0.0203393 -0.0378061 0.0631712 -0.0328202 -0.0356413 -0.150238 -0.21181 -0.0678325 0.104958 -0.0630769 0.396576 -0.048814 -0.097015 -0.0817625 0.00613349 -0.0678604 0.217583 -0.107802 -0.0173628 -0.185115 -0.0451742 0.187445 -0.0416783 0.109618 0.0276893 0.0114261 0.140214 -0.171124 0.113816 0.0269887 -0.104443 -0.131381 ]
ngram: t	hash: 111272	vector square norm: 1.15589	vector: -0.087417 -0.00916161 -0.0496416 0.00214341 0.0793724 0.0126973 -0.00236421 -0.0891492 0.0558969 0.111419 0.0365188 0.163104 -0.0816646 -0.0635446 -0.0258204 0.0307842 0.0826965 -0.017513 0.241828 0.161227 0.126955 -0.0328882 -0.128203 -0.0474052 -0.0749266 -0.0239091 0.17357 0.158426 0.0512094 0.012906 -0.160467 -0.0186447 0.0634632 -0.00857917 0.0576733 0.211892 0.0939939 0.0128043 -0.118022 0.0484567 0.102622 -0.202618 -0.0207809 0.251368 -0.0306068 -0.183734 0.238492 0.13605 -0.122889 -0.0194502 -0.0905502 -0.0732709 -0.0353091 -0.0232906 0.103621 -0.0350835 -0.0712131 0.109374 -0.144369 0.0323849 -0.121751 0.115531 -0.0975247 -0.112304 -0.0767274 0.0970142 0.209366 0.0585102 0.0225534 0.0322295 -0.0470049 0.0349526 0.0212938 0.122796 0.17846 0.0478357 -0.08227 0.0565457 -0.329352 0.0336223 0.07502 0.0654688 -0.00305579 0.0599796 -0.190701 0.0854309 0.00471014 0.160877 0.0451569 -0.158434 0.0285531 -0.087762 -0.0248901 -0.00603825 -0.113946 0.14997 -0.101084 -0.026804 0.0793734 0.117839 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__game 0.998792 __label__house 0.00122831

+++ predictLine

- token(Pine, -1) {getLine->addSubwords}  {out_of_vocab: yes}  { subwords: P i n e  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: P	hash: 337940	vector square norm: 0.00335797	vector: -0.00873132 0.00814062 -0.00365602 -0.000325241 0.00783502 0.00929503 -0.00372966 0.00992815 0.00287027 0.0096397 0.00388409 -0.00133492 -0.000859424 -0.00545385 -0.00111389 0.00387935 -0.00740843 0.00988614 -0.000873172 0.00719881 -0.0095928 -0.00696411 0.00209388 -0.00297003 0.0018571 -0.00822025 -0.00779354 -0.000305382 0.00757872 0.00528728 0.00871212 -0.0094247 -0.00362509 0.00113318 0.000544361 0.00851452 -0.00788398 0.00470185 -0.00745384 0.000195526 -0.00720672 0.00126156 -0.00294419 0.00380863 -0.00871296 -0.00933125 -0.00610064 0.00881946 -0.00500367 0.00358124 -0.000154326 0.000285579 0.00990788 0.00451563 0.00982677 0.0096877 -0.00566977 -0.00419953 0.00404311 0.00418575 -0.00531143 0.00394773 0.00483348 0.00345799 0.00637322 0.00418852 -0.00813815 0.00422762 0.00579165 -0.00213547 0.000256666 -0.00853413 -0.00422118 -0.000147901 -0.000688278 0.00615449 0.00225554 0.00200512 -0.00907884 -0.00872467 -0.00162272 0.00126923 0.00185909 0.00562044 -0.000464622 -0.00239588 0.00576109 0.00692058 -0.00769184 0.00653646 0.00325795 -0.000989206 -0.00575864 0.00518457 -0.00670011 -0.00130931 0.00533562 0.00696132 -0.00494449 -0.00442655 ]
ngram: i	hash: 223177	vector square norm: 2.04729	vector: 0.118312 0.00378389 0.0665222 -0.00767168 -0.106089 -0.0158022 -0.000138476 0.114947 -0.0547025 -0.151036 -0.0426991 -0.214421 0.117983 0.095029 0.0306573 -0.0343082 -0.107125 0.0274153 -0.318735 -0.213259 -0.184339 0.0505264 0.178555 0.0718639 0.101339 0.0304034 -0.226832 -0.204502 -0.0651228 -0.0271916 0.206873 0.0192429 -0.0722212 0.0135979 -0.0715009 -0.27734 -0.115183 -0.0159794 0.15762 -0.0583104 -0.135436 0.27545 0.0199833 -0.328253 0.042952 0.235125 -0.330156 -0.188621 0.167645 0.0328069 0.129442 0.0928168 0.049408 0.0385102 -0.136689 0.0450162 0.0785833 -0.16214 0.186877 -0.0441101 0.153017 -0.161105 0.144472 0.135398 0.092699 -0.148047 -0.296136 -0.0868214 -0.0236096 -0.0491835 0.0646626 -0.0321521 -0.0405789 -0.158477 -0.243502 -0.0718295 0.116179 -0.0667358 0.427495 -0.0422973 -0.106333 -0.0954821 0.0139826 -0.0740106 0.251573 -0.119972 -0.0253623 -0.211729 -0.0512943 0.205013 -0.0274654 0.102909 0.0337411 0.020866 0.160867 -0.194464 0.114668 0.0133141 -0.121244 -0.152814 ]
ngram: n	hash: 1445558	vector square norm: 2.03273	vector: 0.115328 -0.00159127 0.0781617 -0.00968126 -0.109293 -0.0230997 0.0111041 0.112796 -0.0642889 -0.161002 -0.0505491 -0.202014 0.101984 0.0960146 0.028831 -0.0451768 -0.125733 0.0306294 -0.322037 -0.212361 -0.17347 0.0332011 0.167755 0.0793216 0.0888288 0.0476345 -0.224375 -0.197683 -0.0695378 -0.0143966 0.202109 0.0266177 -0.0646274 0.0100877 -0.0849254 -0.266805 -0.126351 -0.0138007 0.143885 -0.0665855 -0.145234 0.275908 0.0209324 -0.326265 0.0368046 0.243728 -0.327991 -0.188467 0.16454 0.0302342 0.122538 0.100268 0.0393594 0.0349506 -0.128555 0.0381871 0.0804341 -0.160807 0.179728 -0.0507491 0.162375 -0.149587 0.146387 0.13092 0.0966589 -0.136515 -0.288352 -0.0970653 -0.0135784 -0.0498898 0.0659459 -0.0328435 -0.0337791 -0.162037 -0.243714 -0.0697479 0.115322 -0.0676911 0.43678 -0.0377829 -0.106804 -0.0880988 -0.000832913 -0.0746874 0.241568 -0.119418 -0.0230651 -0.209441 -0.0493823 0.204185 -0.0427221 0.118767 0.048137 0.0140719 0.155568 -0.198925 0.132848 0.0256785 -0.115287 -0.155781 ]
ngram: e	hash: 891749	vector square norm: 1.67341	vector: 0.0962932 0.0043802 0.0736295 -0.0150743 -0.0951058 -0.014539 0.0146226 0.095285 -0.0662469 -0.143954 -0.0497558 -0.18637 0.0927283 0.0816166 0.0288216 -0.0416271 -0.100454 0.0252866 -0.292181 -0.203267 -0.166384 0.0362609 0.152516 0.0647406 0.0799986 0.0407803 -0.18992 -0.188094 -0.0750006 -0.0188861 0.185696 0.0162367 -0.0727935 0.0158307 -0.0669281 -0.237906 -0.112159 -0.0221323 0.138339 -0.0673749 -0.129141 0.24325 0.0142656 -0.298933 0.0293424 0.223482 -0.295933 -0.178104 0.13813 0.0209466 0.114013 0.0925986 0.0447805 0.0344636 -0.127711 0.0333485 0.0759575 -0.14244 0.174997 -0.0518142 0.133897 -0.143693 0.117515 0.136849 0.0784507 -0.136018 -0.265934 -0.0863082 -0.0203393 -0.0378061 0.0631712 -0.0328202 -0.0356413 -0.150238 -0.21181 -0.0678325 0.104958 -0.0630769 0.396576 -0.048814 -0.097015 -0.0817625 0.00613349 -0.0678604 0.217583 -0.107802 -0.0173628 -0.185115 -0.0451742 0.187445 -0.0416783 0.109618 0.0276893 0.0114261 0.140214 -0.171124 0.113816 0.0269887 -0.104443 -0.131381 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__house 0.999618 __label__game 0.000402337

+++ predictLine

- token(zzzz, -1) {getLine->addSubwords}  {out_of_vocab: yes}  { subwords: z z z z  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: z	hash: 997938	vector square norm: 0.00332322	vector: -0.00963295 0.000641475 0.00149703 -0.00157319 0.00628592 0.00906604 -0.00576281 0.00770714 0.00681379 0.00485156 0.00420891 -0.00298709 0.00970008 -0.00739761 0.00258158 -0.00508457 0.000468719 0.000876954 -0.00806764 0.00575399 0.00588943 0.00730961 -0.00474662 0.00649112 -0.00954476 0.00965096 0.000149325 -0.00871988 0.00686173 0.00902657 0.00595622 0.00768117 -0.00892686 -0.00264898 0.00213053 -0.00760969 -0.00934163 0.00967637 -0.00497654 0.00710835 0.003097 -0.00141452 -0.00440261 0.00197057 0.00593783 0.000629565 0.00647579 -0.00509234 0.0093003 0.00264927 -0.00483387 0.00127716 0.0015164 -0.00610039 0.00210672 0.0067967 0.00817107 -0.00201042 -0.00591064 -0.000748033 -0.00996244 0.00625435 9.38828e-05 0.00917796 -0.00794134 0.00306119 -0.000303146 -0.00837064 0.00292217 -0.00648898 -0.00697691 0.00719861 0.00912769 0.00441456 0.00296839 0.00359283 0.00174933 0.000586919 0.000839744 -0.000606009 0.00581216 0.00112789 0.00854839 0.00071957 0.00259291 -0.00737625 0.00579665 0.00291593 0.00247399 -0.000415634 0.00349685 0.0090363 0.0013072 0.000116483 -0.00212623 0.00343139 -0.000439857 -0.00974854 -0.00686463 0.00461219 ]
ngram: z	hash: 997938	vector square norm: 0.00332322	vector: -0.00963295 0.000641475 0.00149703 -0.00157319 0.00628592 0.00906604 -0.00576281 0.00770714 0.00681379 0.00485156 0.00420891 -0.00298709 0.00970008 -0.00739761 0.00258158 -0.00508457 0.000468719 0.000876954 -0.00806764 0.00575399 0.00588943 0.00730961 -0.00474662 0.00649112 -0.00954476 0.00965096 0.000149325 -0.00871988 0.00686173 0.00902657 0.00595622 0.00768117 -0.00892686 -0.00264898 0.00213053 -0.00760969 -0.00934163 0.00967637 -0.00497654 0.00710835 0.003097 -0.00141452 -0.00440261 0.00197057 0.00593783 0.000629565 0.00647579 -0.00509234 0.0093003 0.00264927 -0.00483387 0.00127716 0.0015164 -0.00610039 0.00210672 0.0067967 0.00817107 -0.00201042 -0.00591064 -0.000748033 -0.00996244 0.00625435 9.38828e-05 0.00917796 -0.00794134 0.00306119 -0.000303146 -0.00837064 0.00292217 -0.00648898 -0.00697691 0.00719861 0.00912769 0.00441456 0.00296839 0.00359283 0.00174933 0.000586919 0.000839744 -0.000606009 0.00581216 0.00112789 0.00854839 0.00071957 0.00259291 -0.00737625 0.00579665 0.00291593 0.00247399 -0.000415634 0.00349685 0.0090363 0.0013072 0.000116483 -0.00212623 0.00343139 -0.000439857 -0.00974854 -0.00686463 0.00461219 ]
ngram: z	hash: 997938	vector square norm: 0.00332322	vector: -0.00963295 0.000641475 0.00149703 -0.00157319 0.00628592 0.00906604 -0.00576281 0.00770714 0.00681379 0.00485156 0.00420891 -0.00298709 0.00970008 -0.00739761 0.00258158 -0.00508457 0.000468719 0.000876954 -0.00806764 0.00575399 0.00588943 0.00730961 -0.00474662 0.00649112 -0.00954476 0.00965096 0.000149325 -0.00871988 0.00686173 0.00902657 0.00595622 0.00768117 -0.00892686 -0.00264898 0.00213053 -0.00760969 -0.00934163 0.00967637 -0.00497654 0.00710835 0.003097 -0.00141452 -0.00440261 0.00197057 0.00593783 0.000629565 0.00647579 -0.00509234 0.0093003 0.00264927 -0.00483387 0.00127716 0.0015164 -0.00610039 0.00210672 0.0067967 0.00817107 -0.00201042 -0.00591064 -0.000748033 -0.00996244 0.00625435 9.38828e-05 0.00917796 -0.00794134 0.00306119 -0.000303146 -0.00837064 0.00292217 -0.00648898 -0.00697691 0.00719861 0.00912769 0.00441456 0.00296839 0.00359283 0.00174933 0.000586919 0.000839744 -0.000606009 0.00581216 0.00112789 0.00854839 0.00071957 0.00259291 -0.00737625 0.00579665 0.00291593 0.00247399 -0.000415634 0.00349685 0.0090363 0.0013072 0.000116483 -0.00212623 0.00343139 -0.000439857 -0.00974854 -0.00686463 0.00461219 ]
ngram: z	hash: 997938	vector square norm: 0.00332322	vector: -0.00963295 0.000641475 0.00149703 -0.00157319 0.00628592 0.00906604 -0.00576281 0.00770714 0.00681379 0.00485156 0.00420891 -0.00298709 0.00970008 -0.00739761 0.00258158 -0.00508457 0.000468719 0.000876954 -0.00806764 0.00575399 0.00588943 0.00730961 -0.00474662 0.00649112 -0.00954476 0.00965096 0.000149325 -0.00871988 0.00686173 0.00902657 0.00595622 0.00768117 -0.00892686 -0.00264898 0.00213053 -0.00760969 -0.00934163 0.00967637 -0.00497654 0.00710835 0.003097 -0.00141452 -0.00440261 0.00197057 0.00593783 0.000629565 0.00647579 -0.00509234 0.0093003 0.00264927 -0.00483387 0.00127716 0.0015164 -0.00610039 0.00210672 0.0067967 0.00817107 -0.00201042 -0.00591064 -0.000748033 -0.00996244 0.00625435 9.38828e-05 0.00917796 -0.00794134 0.00306119 -0.000303146 -0.00837064 0.00292217 -0.00648898 -0.00697691 0.00719861 0.00912769 0.00441456 0.00296839 0.00359283 0.00174933 0.000586919 0.000839744 -0.000606009 0.00581216 0.00112789 0.00854839 0.00071957 0.00259291 -0.00737625 0.00579665 0.00291593 0.00247399 -0.000415634 0.00349685 0.0090363 0.0013072 0.000116483 -0.00212623 0.00343139 -0.000439857 -0.00974854 -0.00686463 0.00461219 ]
ngram: </s>	hash: 0	vector square norm: 0.410555	vector: 0.0441357 0.00850446 0.0269607 -0.00677628 -0.0458059 -0.00638018 0.00866818 0.0477718 -0.0197432 -0.0617869 -0.0239338 -0.0928201 0.0594919 0.049235 0.0049409 -0.0214992 -0.0527731 0.0204955 -0.14207 -0.103547 -0.0829544 0.0258788 0.0840251 0.0238303 0.0471347 0.010882 -0.0992795 -0.0935037 -0.0332505 -0.00103125 0.0835762 0.0085684 -0.0402739 0.00112405 -0.0426472 -0.123126 -0.0489979 -0.0136306 0.0673738 -0.0357495 -0.0504381 0.126039 0.0118158 -0.143277 0.0133389 0.10993 -0.148313 -0.08914 0.0646604 0.0153677 0.0492736 0.0529371 0.0121728 0.0200496 -0.0606288 0.0166007 0.0429533 -0.0658687 0.0861695 -0.0199082 0.0643294 -0.0653641 0.0681661 0.0694361 0.0404002 -0.0641109 -0.132894 -0.0309538 0.000390964 -0.0137318 0.0179443 -0.0205108 -0.0200514 -0.0720067 -0.106609 -0.034734 0.0556311 -0.0210166 0.192059 -0.0246654 -0.0514547 -0.0342866 0.00993249 -0.0432353 0.104235 -0.0571101 -0.00536772 -0.0910998 -0.0264662 0.0916992 -0.00710089 0.0541099 0.0263339 -0.00064811 0.0680925 -0.0922506 0.0636266 0.0169942 -0.0540427 -0.0690531 ]

vector square norms: ;
__label__house 0.738515 __label__game 0.261505
```


In this example, we see that the word "zzzz" is predicted almost equally as a house or game category, because:
- the word is not in the training set, so no vector associated.
- none of its ngrams are in the training set, so their vectors have very low norm.

The word "Pine" is predicted as a house category, as it has 2 ngrans with big vector square norm from the house category, and 1 ngram shared by both categories with big vector square norm:
- i from kitchen (house category)
- n from kitchen (house category)
- e from bed (house category), kitchen (house category) and basket (game category)



## Example 2
```
$ ./fasttext supervised -minn 0 -maxn 0 -wordNgrams 0 -epoch 500 -lr 0.5 -input train.txt -output model

$ ./fasttext predict-prob model.bin test.txt 2


+++ predictLine

- token(bed, 1) {getLine->addSubwords}  {out_of_vocab: no}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: bed	hash: 1	vector square norm: 6.35685	vector: 0.334785 -0.46854 0.361599 -0.163483 -0.119391 -0.385331 -0.27222 0.0463541 -0.249601 -0.216316 -0.0631903 -0.341789 0.0700049 -0.30403 -0.445713 0.0974894 0.23419 0.26004 -0.376278 -0.131808 -0.338132 -0.165078 0.00282035 0.386182 -0.0969561 -0.286287 -0.502112 -0.17176 -0.0779557 -0.19951 -0.259133 0.179439 0.246222 -0.185402 -0.160338 -0.512412 -0.1881 0.253266 0.104379 0.0366171 -0.41263 0.256442 -0.0888166 0.0381279 0.21622 0.417985 0.172594 -0.0201747 0.0303211 0.0636773 0.371442 -0.0748859 0.108105 0.118242 -0.482607 0.265147 -0.0562507 -0.332155 0.0785072 -0.355582 0.39806 0.0712771 0.17686 0.472305 0.0471864 -0.18523 -0.101351 0.117041 0.0324372 -0.301973 0.107536 0.109992 0.344737 -0.578022 -0.113734 -0.0934061 0.00329473 -0.159678 0.0357141 0.25591 0.382206 -0.341123 -0.402235 -0.165801 -0.140955 0.076992 -0.106506 -0.332067 0.39999 0.198892 -0.294934 0.139666 -0.113496 -0.00990808 0.34776 0.059921 -0.112951 0.111352 0.220048 0.099879 ]
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__house 0.999657 __label__game 0.000363286

+++ predictLine

- token(kitchen, 2) {getLine->addSubwords}  {out_of_vocab: no}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: kitchen	hash: 2	vector square norm: 6.36275	vector: 0.339388 -0.479431 0.353832 -0.158951 -0.125431 -0.398288 -0.274781 0.0382724 -0.239248 -0.207334 -0.0709042 -0.337779 0.0721625 -0.31092 -0.443345 0.106909 0.232851 0.252314 -0.376022 -0.129015 -0.328496 -0.17805 -0.00550854 0.383627 -0.0909748 -0.28219 -0.490207 -0.178773 -0.083999 -0.192785 -0.252469 0.1869 0.240095 -0.177418 -0.162109 -0.51671 -0.187692 0.256385 0.111528 0.0460753 -0.410521 0.252403 -0.0815324 0.0344686 0.211769 0.420544 0.173147 -0.0117332 0.0390961 0.067238 0.370856 -0.0746296 0.110537 0.109051 -0.4836 0.267935 -0.0464774 -0.339584 0.0666685 -0.35756 0.405275 0.0751129 0.177859 0.470305 0.0374737 -0.179866 -0.0993837 0.119646 0.0405099 -0.304965 0.11306 0.108806 0.343129 -0.579769 -0.118695 -0.0877239 0.0141548 -0.16412 0.0378023 0.244597 0.389448 -0.338289 -0.409739 -0.171878 -0.145807 0.0688176 -0.10693 -0.31598 0.408723 0.188912 -0.302725 0.136536 -0.110651 -0.0134738 0.341905 0.0563417 -0.114135 0.114163 0.230615 0.113386 ]
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__house 0.999658 __label__game 0.000362118

+++ predictLine

- token(football, 4) {getLine->addSubwords}  {out_of_vocab: no}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: football	hash: 4	vector square norm: 6.4507	vector: -0.336958 0.478501 -0.355961 0.159917 0.132275 0.403064 0.271622 -0.0540751 0.2503 0.216947 0.0635621 0.334697 -0.0819088 0.308453 0.454375 -0.1027 -0.231309 -0.261718 0.368971 0.143417 0.345141 0.169235 -0.00559821 -0.390354 0.0918032 0.289786 0.506615 0.17714 0.0842712 0.198546 0.264492 -0.188667 -0.249497 0.182804 0.160856 0.521717 0.174709 -0.240664 -0.0997529 -0.0363044 0.401507 -0.255918 0.0912896 -0.0279593 -0.205193 -0.429598 -0.167108 0.0271984 -0.0327832 -0.0652575 -0.377357 0.0759645 -0.100121 -0.116487 0.48824 -0.255926 0.0439721 0.339216 -0.0823082 0.363784 -0.396354 -0.0692425 -0.180085 -0.472556 -0.0360302 0.188527 0.0948064 -0.13198 -0.0451237 0.301839 -0.107818 -0.107888 -0.348325 0.582791 0.108253 0.0894567 -0.0159827 0.15128 -0.0267596 -0.254644 -0.389631 0.339548 0.405473 0.175812 0.147138 -0.0748087 0.108066 0.332465 -0.410801 -0.186964 0.302166 -0.1401 0.121799 0.0139653 -0.342683 -0.0487401 0.11122 -0.122135 -0.231876 -0.111613 ]
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__game 0.999687 __label__house 0.000333307

+++ predictLine

- token(basket, 3) {getLine->addSubwords}  {out_of_vocab: no}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: basket	hash: 3	vector square norm: 6.19545	vector: -0.327145 0.473664 -0.353298 0.150377 0.125114 0.394608 0.257407 -0.0464211 0.247863 0.216467 0.0580604 0.337468 -0.080958 0.304701 0.439848 -0.106711 -0.232906 -0.258237 0.36997 0.134085 0.337262 0.177304 -0.000806484 -0.373883 0.0818784 0.281731 0.493763 0.166998 0.0683309 0.186018 0.257921 -0.173927 -0.239923 0.184982 0.151733 0.510543 0.178991 -0.246661 -0.103583 -0.0340686 0.398281 -0.246061 0.0804832 -0.0393607 -0.204913 -0.415937 -0.156557 0.0177064 -0.0289755 -0.0550504 -0.375585 0.0670918 -0.10174 -0.118535 0.468438 -0.261487 0.0457847 0.331482 -0.0678243 0.350521 -0.398754 -0.0653753 -0.168329 -0.466986 -0.0468275 0.182291 0.0945071 -0.126627 -0.0401277 0.289472 -0.0969695 -0.102059 -0.335226 0.568296 0.109358 0.0968118 -0.00491878 0.165461 -0.0231798 -0.253756 -0.375983 0.333664 0.405101 0.17211 0.151914 -0.072234 0.102634 0.32606 -0.407121 -0.195763 0.294406 -0.141118 0.115252 0.0149968 -0.339896 -0.0582919 0.110446 -0.106511 -0.22559 -0.103692 ]
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__game 0.999631 __label__house 0.00038947

+++ predictLine

- token(Pine, -1) {getLine->addSubwords}  {out_of_vocab: yes}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__game 0.507608 __label__house 0.492412

+++ predictLine

- token(zzzz, -1) {getLine->addSubwords}  {out_of_vocab: yes}  { subwords:  }
- token(</s>, 0) {getLine->addSubwords}  {out_of_vocab: no}
ngrams:
ngram: </s>	hash: 0	vector square norm: 0.00116515	vector: -0.00413725 0.00104449 -0.0017458 0.00053561 0.000374783 -0.0025346 0.00490257 0.00226653 0.00165752 0.00141426 0.00136175 0.000367164 0.00721498 0.00226523 -0.00387253 0.000502404 0.00106049 0.00488826 0.000296114 -0.00602766 -0.00424892 0.00146433 0.0035699 -0.00258049 0.0045106 -0.00422866 -0.00241982 -0.000733883 0.00138865 0.00535033 -0.00579819 -0.00242295 -0.0027109 -0.00526283 -0.00315026 -0.00282945 0.00303644 -0.00502001 -0.00190605 -0.00499171 0.00597187 0.00291502 0.00164263 0.00134854 -0.00355915 0.00249137 -0.00523119 -0.00344304 -0.00524999 -0.00110862 -0.000601146 0.00537076 -0.00604842 0.00408384 0.00124095 -0.0035188 0.00171117 0.00102597 0.00200776 0.00139752 -0.00290606 0.000321269 0.0015453 0.00292671 -0.000405416 -0.000815274 -0.00190082 0.00547989 0.00472338 0.00498845 -0.00734752 -0.00123131 -0.00233472 0.000915014 0.00187771 -0.00354257 0.00339573 0.00377609 -0.00414477 0.000311313 -0.00542446 0.00324256 0.00401806 -0.00501977 -0.00439118 -0.00291493 0.00102851 -0.00165281 -0.000829435 -0.00096433 0.00528048 0.00291739 0.0026915 -0.0046592 -0.000531781 -0.00424863 0.0037168 0.00316872 -0.00268734 -0.00194901 ]

vector square norms: ;
__label__game 0.507608 __label__house 0.492412
```



# fastText
[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.

[![CircleCI](https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg)](https://circleci.com/gh/facebookresearch/fastText/tree/master)

## Table of contents

* [Resources](#resources)
   * [Models](#models)
   * [Supplementary data](#supplementary-data)
   * [FAQ](#faq)
   * [Cheatsheet](#cheatsheet)
* [Requirements](#requirements)
* [Building fastText](#building-fasttext)
   * [Getting the source code](#getting-the-source-code)
   * [Building fastText using make (preferred)](#building-fasttext-using-make-preferred)
   * [Building fastText using cmake](#building-fasttext-using-cmake)
   * [Building fastText for Python](#building-fasttext-for-python)
* [Example use cases](#example-use-cases)
   * [Word representation learning](#word-representation-learning)
   * [Obtaining word vectors for out-of-vocabulary words](#obtaining-word-vectors-for-out-of-vocabulary-words)
   * [Text classification](#text-classification)
* [Full documentation](#full-documentation)
* [References](#references)
   * [Enriching Word Vectors with Subword Information](#enriching-word-vectors-with-subword-information)
   * [Bag of Tricks for Efficient Text Classification](#bag-of-tricks-for-efficient-text-classification)
   * [FastText.zip: Compressing text classification models](#fasttextzip-compressing-text-classification-models)
* [Join the fastText community](#join-the-fasttext-community)
* [License](#license)

## Resources

### Models
- Recent state-of-the-art [English word vectors](https://fasttext.cc/docs/en/english-vectors.html).
- Word vectors for [157 languages trained on Wikipedia and Crawl](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md).
- Models for [language identification](https://fasttext.cc/docs/en/language-identification.html#content) and [various supervised tasks](https://fasttext.cc/docs/en/supervised-models.html#content).

### Supplementary data
- The preprocessed [YFCC100M data](https://fasttext.cc/docs/en/dataset.html#content) used in [2].

### FAQ

You can find [answers to frequently asked questions](https://fasttext.cc/docs/en/faqs.html#content) on our [website](https://fasttext.cc/).

### Cheatsheet

We also provide a [cheatsheet](https://fasttext.cc/docs/en/cheatsheet.html#content) full of useful one-liners.

## Requirements

We are continuously building and testing our library, CLI and Python bindings under various docker images using [circleci](https://circleci.com/).

Generally, **fastText** builds on modern Mac OS and Linux distributions.
Since it uses some C++11 features, it requires a compiler with good C++11 support.
These include :

* (g++-4.7.2 or newer) or (clang-3.3 or newer)

Compilation is carried out using a Makefile, so you will need to have a working **make**.
If you want to use **cmake** you need at least version 2.8.9.

One of the oldest distributions we successfully built and tested the CLI under is [Debian jessie](https://www.debian.org/releases/jessie/).

For the word-similarity evaluation script you will need:

* Python 2.6 or newer
* NumPy & SciPy

For the python bindings (see the subdirectory python) you will need:

* Python version 2.7 or >=3.4
* NumPy & SciPy
* [pybind11](https://github.com/pybind/pybind11)

One of the oldest distributions we successfully built and tested the Python bindings under is [Debian jessie](https://www.debian.org/releases/jessie/).

If these requirements make it impossible for you to use fastText, please open an issue and we will try to accommodate you.

## Building fastText

We discuss building the latest stable version of fastText.

### Getting the source code

You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.

There is also the master branch that contains all of our most recent work, but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.

### Building fastText using make (preferred)

```
$ wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip
$ unzip v0.2.0.zip
$ cd fastText-0.2.0
$ make
```

This will produce object files for all the classes as well as the main binary `fasttext`.
If you do not plan on using the default system-wide compiler, update the two macros defined at the beginning of the Makefile (CC and INCLUDES).

### Building fastText using cmake

For now this is not part of a release, so you will need to clone the master branch.

```
$ git clone https://github.com/facebookresearch/fastText.git
$ cd fastText
$ mkdir build && cd build && cmake ..
$ make && make install
```

This will create the fasttext binary and also all relevant libraries (shared, static, PIC).

### Building fastText for Python

For now this is not part of a release, so you will need to clone the master branch.

```
$ git clone https://github.com/facebookresearch/fastText.git
$ cd fastText
$ pip install .
```

For further information and introduction see python/README.md

## Example use cases

This library has two main use cases: word representation learning and text classification.
These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).

### Word representation learning

In order to learn word vectors, as described in [1](#enriching-word-vectors-with-subword-information), do:

```
$ ./fasttext skipgram -input data.txt -output model
```

where `data.txt` is a training file containing `UTF-8` encoded text.
By default the word vectors will take into account character n-grams from 3 to 6 characters.
At the end of optimization the program will save two files: `model.bin` and `model.vec`.
`model.vec` is a text file containing the word vectors, one per line.
`model.bin` is a binary file containing the parameters of the model along with the dictionary and all hyper parameters.
The binary file can be used later to compute word vectors or to restart the optimization.

### Obtaining word vectors for out-of-vocabulary words

The previously trained model can be used to compute word vectors for out-of-vocabulary words.
Provided you have a text file `queries.txt` containing words for which you want to compute vectors, use the following command:

```
$ ./fasttext print-word-vectors model.bin < queries.txt
```

This will output word vectors to the standard output, one vector per line.
This can also be used with pipes:

```
$ cat queries.txt | ./fasttext print-word-vectors model.bin
```

See the provided scripts for an example. For instance, running:

```
$ ./word-vector-example.sh
```

will compile the code, download data, compute word vectors and evaluate them on the rare words similarity dataset RW [Thang et al. 2013].

### Text classification

This library can also be used to train supervised text classifiers, for instance for sentiment analysis.
In order to train a text classifier using the method described in [2](#bag-of-tricks-for-efficient-text-classification), use:

```
$ ./fasttext supervised -input train.txt -output model
```

where `train.txt` is a text file containing a training sentence per line along with the labels.
By default, we assume that labels are words that are prefixed by the string `__label__`.
This will output two files: `model.bin` and `model.vec`.
Once the model was trained, you can evaluate it by computing the precision and recall at k (P@k and R@k) on a test set using:

```
$ ./fasttext test model.bin test.txt k
```

The argument `k` is optional, and is equal to `1` by default.

In order to obtain the k most likely labels for a piece of text, use:

```
$ ./fasttext predict model.bin test.txt k
```

or use `predict-prob` to also get the probability for each label

```
$ ./fasttext predict-prob model.bin test.txt k
```

where `test.txt` contains a piece of text to classify per line.
Doing so will print to the standard output the k most likely labels for each line.
The argument `k` is optional, and equal to `1` by default.
See `classification-example.sh` for an example use case.
In order to reproduce results from the paper [2](#bag-of-tricks-for-efficient-text-classification), run `classification-results.sh`, this will download all the datasets and reproduce the results from Table 1.

If you want to compute vector representations of sentences or paragraphs, please use:

```
$ ./fasttext print-sentence-vectors model.bin < text.txt
```

This assumes that the `text.txt` file contains the paragraphs that you want to get vectors for.
The program will output one vector representation per line in the file.

You can also quantize a supervised model to reduce its memory usage with the following command:

```
$ ./fasttext quantize -output model
```
This will create a `.ftz` file with a smaller memory footprint. All the standard functionality, like `test` or `predict` work the same way on the quantized models:
```
$ ./fasttext test model.ftz test.txt
```
The quantization procedure follows the steps described in [3](#fasttextzip-compressing-text-classification-models). You can
run the script `quantization-example.sh` for an example.


## Full documentation

Invoke a command without arguments to list available arguments and their default values:

```
$ ./fasttext supervised
Empty input or output path.

The following arguments are mandatory:
  -input              training file path
  -output             output file path

The following arguments are optional:
  -verbose            verbosity level [2]

The following arguments for the dictionary are optional:
  -minCount           minimal number of word occurrences [1]
  -minCountLabel      minimal number of label occurrences [0]
  -wordNgrams         max length of word ngram [1]
  -bucket             number of buckets [2000000]
  -minn               min length of char ngram [0]
  -maxn               max length of char ngram [0]
  -t                  sampling threshold [0.0001]
  -label              labels prefix [__label__]

The following arguments for training are optional:
  -lr                 learning rate [0.1]
  -lrUpdateRate       change the rate of updates for the learning rate [100]
  -dim                size of word vectors [100]
  -ws                 size of the context window [5]
  -epoch              number of epochs [5]
  -neg                number of negatives sampled [5]
  -loss               loss function {ns, hs, softmax} [softmax]
  -thread             number of threads [12]
  -pretrainedVectors  pretrained word vectors for supervised learning []
  -saveOutput         whether output params should be saved [0]

The following arguments for quantization are optional:
  -cutoff             number of words and ngrams to retain [0]
  -retrain            finetune embeddings if a cutoff is applied [0]
  -qnorm              quantizing the norm separately [0]
  -qout               quantizing the classifier [0]
  -dsub               size of each sub-vector [2]
```

Defaults may vary by mode. (Word-representation modes `skipgram` and `cbow` use a default `-minCount` of 5.)

## References

Please cite [1](#enriching-word-vectors-with-subword-information) if using this code for learning word representations or [2](#bag-of-tricks-for-efficient-text-classification) if using for text classification.

### Enriching Word Vectors with Subword Information

[1] P. Bojanowski\*, E. Grave\*, A. Joulin, T. Mikolov, [*Enriching Word Vectors with Subword Information*](https://arxiv.org/abs/1607.04606)

```
@article{bojanowski2017enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  year={2017},
  issn={2307-387X},
  pages={135--146}
}
```

### Bag of Tricks for Efficient Text Classification

[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [*Bag of Tricks for Efficient Text Classification*](https://arxiv.org/abs/1607.01759)

```
@InProceedings{joulin2017bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month={April},
  year={2017},
  publisher={Association for Computational Linguistics},
  pages={427--431},
}
```

### FastText.zip: Compressing text classification models

[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, [*FastText.zip: Compressing text classification models*](https://arxiv.org/abs/1612.03651)

```
@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}
```

(\* These authors contributed equally.)


## Join the fastText community

* Facebook page: https://www.facebook.com/groups/1174547215919768
* Google group: https://groups.google.com/forum/#!forum/fasttext-library
* Contact: [egrave@fb.com](mailto:egrave@fb.com), [bojanowski@fb.com](mailto:bojanowski@fb.com), [ajoulin@fb.com](mailto:ajoulin@fb.com), [tmikolov@fb.com](mailto:tmikolov@fb.com)

See the CONTRIBUTING file for information about how to help out.

## License

fastText is MIT-licensed.
